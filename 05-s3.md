# s3

mood music: [quiet mornings](https://www.youtube.com/watch?v=ecechHEtkYU)

[s3 overview](https://www.udemy.com/course/aws-certified-solutions-architect-associate-saa-c03/learn/lecture/13528314#lecture-article) + [s3 hands on](https://www.udemy.com/course/aws-certified-solutions-architect-associate-saa-c03/learn/lecture/23743312#lecture-article)
* files are stored as objects within region-specific buckets requiring globally unique names
* key = prefix and object name (although no true directories) - `s3://bucket/prefix/file.extension`
* object value = content of the body...
* file size limit: 5tb; multipart upload required for >5gb
* can tag, version, etc metadata

[s3 security](https://www.udemy.com/course/aws-certified-solutions-architect-associate-saa-c03/learn/lecture/13528320#lecture-article) + [hands on](https://www.udemy.com/course/aws-certified-solutions-architect-associate-saa-c03/learn/lecture/19736414#lecture-article)
* iam policy / role - json as usual
* resource based bucket policy - can cross account; object access control list, bucket access control list
* iam OR resource AND no explict deny
* encryption using keys
* note in TF you don't actually have to policy for the most common stuff: 

```
resource "aws_s3_bucket_public_access_block" "lambda_response_bucket_pab" {
  bucket = aws_s3_bucket.lambda_response_bucket.id

  block_public_acls       = true
  block_public_policy     = true
  ignore_public_acls      = true
  restrict_public_buckets = true
}
```

[s3 website overview](https://www.udemy.com/course/aws-certified-solutions-architect-associate-saa-c03/learn/lecture/13528322#lecture-article) + [hands on](https://www.udemy.com/course/aws-certified-solutions-architect-associate-saa-c03/learn/lecture/34352950#lecture-article)
* blah

[s3 versioning](https://www.udemy.com/course/aws-certified-solutions-architect-associate-saa-c03/learn/lecture/13528316#lecture-article) + [hands on](https://www.udemy.com/course/aws-certified-solutions-architect-associate-saa-c03/learn/lecture/23743314#lecture-article)
* enabled at the bucket level; null version if file was there before versioning enabled. suspending versioning doesn't delete previous versions
* delete markers - actual previous versions are stil inside bucket, remove market to restore access to file

[replication](https://www.udemy.com/course/aws-certified-solutions-architect-associate-saa-c03/learn/lecture/34353002#overview) + [notes](https://www.udemy.com/course/aws-certified-solutions-architect-associate-saa-c03/learn/lecture/34353008#lecture-article) + [hands on](https://www.udemy.com/course/aws-certified-solutions-architect-associate-saa-c03/learn/lecture/23743374#lecture-article)
* replication - only works if versioning enabled on both source and target. 
* cross region vs same region replication
* iam role required for replication
* no replicaion chaining
* permanent deletions with version IDs are not replicated, delete markers can be optionally replicated
* `S3 Batch Replication` to replicate existing objects
* replication rules

[storage classes overview](https://www.udemy.com/course/aws-certified-solutions-architect-associate-saa-c03/learn/lecture/13528350#lecture-article) + [hands on](https://www.udemy.com/course/aws-certified-solutions-architect-associate-saa-c03/learn/lecture/23743396#lecture-article)
* Amazon S3 Standard - General Purpose
* Amazon S3 Infrequent Access (Standard-IA) - Offers 99.9% availability, cost on retrieval
* Amazon S3 One Zone-Infrequent Access (One Zone-IA) - Offers 99.5% availability
* Glacier Instant Retrieval - milisecond retreival
* Glacier Flexible Retrieval - expedited (1-5min), standard (3-5h), bulk (5-12h) is free. 90 days min storage duration
* Glacier Deep Archive - standard (12h) vs bulk (48h), 180 days min storage duration
* Amazon S3 Intelligent-Tiering
* 11 9s durability -  if you store 10 million objects, you can expect to lose only one object every 10,000 years on average; availability depends on the storage class

[s3 lifecycle rules with analytics](https://www.udemy.com/course/aws-certified-solutions-architect-associate-saa-c03/learn/lecture/13528352#lecture-article) + [hands on](https://www.udemy.com/course/aws-certified-solutions-architect-associate-saa-c03/learn/lecture/23743416#lecture-article)
* not everything is possible, generally if it's standard it can go anywhere, but deep archive cannot come back for e.g. once moved into a lower tier, cannot come back
* lifecycle rules (can be applied to **buckets, specific prefixes or object tags**)
    * transition actions
    * expiration actions
    * incomplete multipart actions
* s3 analytics - recommendations for transitions between Standard and Standard IA storage classes. does not support one zone ia or galcier, generates a csv report with stats and recs

[s3 requester pays](https://www.udemy.com/course/aws-certified-solutions-architect-associate-saa-c03/learn/lecture/26099260#lecture-article)
* bucket owners pay for storage + data transfer costs
* enabling requester pays - which makes the requester pay for **networking costs**. user *must* be authenticated in AWS

[s3 event notifications](https://www.udemy.com/course/aws-certified-solutions-architect-associate-saa-c03/learn/lecture/19736478#lecture-article)
* use case - trigger automated workflows
* send to sns topic, sqs queue, lambda function. unlimited s3 event notifications. delivery takes seconds to minutes (so... slow)
* need to configure appropriate iam permissions -- **resource policies** directly attached to the topic queue or function
* amazon eventbridge also can - all events are sent to eventbridge anyway. **advanced filtering**, send to **multiple destinations**, **event archiving, replay, more reliable delivery**.
* instead of directly writing to sqs queue from lambda, i can probably lambda -> save to s3 -> trigger event either directly to lambda or into an sqs queue

[s3 performance](https://www.udemy.com/course/aws-certified-solutions-architect-associate-saa-c03/learn/lecture/18078239#lecture-article)
* each prefix in an S3 bucket supports:
    * 3,500 PUT, COPY, POST, or DELETE requests per second
    * 5,500 GET or HEAD requests per second
* ***s3 transfer acceleration*** -  **increases upload and download speeds by routing data through AWS edge locations**. basically within aws it's fast, minimise edge latency so can reduce bottleneck
* ***s3 byte range fetches*** - allows parallelisation / partial retrieval of the file / speed up downloads

[s3 batch operations](https://www.udemy.com/course/aws-certified-solutions-architect-associate-saa-c03/learn/lecture/33878726#lecture-article)
* Modify all the object metadata and properties of many S3 objects at once.
* Copy objects between S3 buckets as a batch operation.
* Encrypt all unencrypted objects in your S3 buckets.
* Modify Access Control Lists (ACLs) or tags for multiple objects.
* Restore many objects simultaneously from S3 Glacier.
* Invoke a Lambda function to perform custom actions on every object in the batch.
