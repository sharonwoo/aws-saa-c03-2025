# s3

mood music: [quiet mornings](https://www.youtube.com/watch?v=ecechHEtkYU)

[s3 overview](https://www.udemy.com/course/aws-certified-solutions-architect-associate-saa-c03/learn/lecture/13528314#lecture-article) + [s3 hands on](https://www.udemy.com/course/aws-certified-solutions-architect-associate-saa-c03/learn/lecture/23743312#lecture-article)
* files are stored as objects within region-specific buckets requiring globally unique names
* key = prefix and object name (although no true directories) - `s3://bucket/prefix/file.extension`
* object value = content of the body...
* file size limit: 5tb; multipart upload required for >5gb
* can tag, version, etc metadata

[s3 security](https://www.udemy.com/course/aws-certified-solutions-architect-associate-saa-c03/learn/lecture/13528320#lecture-article) + [hands on](https://www.udemy.com/course/aws-certified-solutions-architect-associate-saa-c03/learn/lecture/19736414#lecture-article)
* iam policy / role - json as usual
* resource based bucket policy - can cross account; object access control list, bucket access control list
* iam OR resource AND no explict deny
* encryption using keys
* note in TF you don't actually have to policy for the most common stuff: 

```
resource "aws_s3_bucket_public_access_block" "lambda_response_bucket_pab" {
  bucket = aws_s3_bucket.lambda_response_bucket.id

  block_public_acls       = true
  block_public_policy     = true
  ignore_public_acls      = true
  restrict_public_buckets = true
}
```

[s3 website overview](https://www.udemy.com/course/aws-certified-solutions-architect-associate-saa-c03/learn/lecture/13528322#lecture-article) + [hands on](https://www.udemy.com/course/aws-certified-solutions-architect-associate-saa-c03/learn/lecture/34352950#lecture-article)
* blah

[s3 versioning](https://www.udemy.com/course/aws-certified-solutions-architect-associate-saa-c03/learn/lecture/13528316#lecture-article) + [hands on](https://www.udemy.com/course/aws-certified-solutions-architect-associate-saa-c03/learn/lecture/23743314#lecture-article)
* enabled at the bucket level; null version if file was there before versioning enabled. suspending versioning doesn't delete previous versions
* delete markers - actual previous versions are stil inside bucket, remove market to restore access to file

[replication](https://www.udemy.com/course/aws-certified-solutions-architect-associate-saa-c03/learn/lecture/34353002#overview) + [notes](https://www.udemy.com/course/aws-certified-solutions-architect-associate-saa-c03/learn/lecture/34353008#lecture-article) + [hands on](https://www.udemy.com/course/aws-certified-solutions-architect-associate-saa-c03/learn/lecture/23743374#lecture-article)
* replication - only works if versioning enabled on both source and target. 
* cross region vs same region replication
* iam role required for replication
* no replicaion chaining
* permanent deletions with version IDs are not replicated, delete markers can be optionally replicated
* `S3 Batch Replication` to replicate existing objects
* replication rules

[storage classes overview](https://www.udemy.com/course/aws-certified-solutions-architect-associate-saa-c03/learn/lecture/13528350#lecture-article) + [hands on](https://www.udemy.com/course/aws-certified-solutions-architect-associate-saa-c03/learn/lecture/23743396#lecture-article)
* Amazon S3 Standard - General Purpose
* Amazon S3 Infrequent Access (Standard-IA) - Offers 99.9% availability, cost on retrieval
* Amazon S3 One Zone-Infrequent Access (One Zone-IA) - Offers 99.5% availability
* Glacier Instant Retrieval - milisecond retreival
* Glacier Flexible Retrieval - expedited (1-5min), standard (3-5h), bulk (5-12h) is free. 90 days min storage duration
* Glacier Deep Archive - standard (12h) vs bulk (48h), 180 days min storage duration
* Amazon S3 Intelligent-Tiering
* 11 9s durability -  if you store 10 million objects, you can expect to lose only one object every 10,000 years on average; availability depends on the storage class

[s3 lifecycle rules with analytics](https://www.udemy.com/course/aws-certified-solutions-architect-associate-saa-c03/learn/lecture/13528352#lecture-article) + [hands on](https://www.udemy.com/course/aws-certified-solutions-architect-associate-saa-c03/learn/lecture/23743416#lecture-article)
* not everything is possible, generally if it's standard it can go anywhere, but deep archive cannot come back for e.g. once moved into a lower tier, cannot come back
* lifecycle rules (can be applied to **buckets, specific prefixes or object tags**)
    * transition actions
    * expiration actions
    * incomplete multipart actions
* s3 analytics - recommendations for transitions between Standard and Standard IA storage classes. does not support one zone ia or galcier, generates a csv report with stats and recs

[s3 requester pays](https://www.udemy.com/course/aws-certified-solutions-architect-associate-saa-c03/learn/lecture/26099260#lecture-article)
* bucket owners pay for storage + data transfer costs
* enabling requester pays - which makes the requester pay for **networking costs**. user *must* be authenticated in AWS

[s3 event notifications](https://www.udemy.com/course/aws-certified-solutions-architect-associate-saa-c03/learn/lecture/19736478#lecture-article)
* use case - trigger automated workflows
* send to sns topic, sqs queue, lambda function. unlimited s3 event notifications. delivery takes seconds to minutes (so... slow)
* need to configure appropriate iam permissions -- **resource policies** directly attached to the topic queue or function
* amazon eventbridge also can - all events are sent to eventbridge anyway. **advanced filtering**, send to **multiple destinations**, **event archiving, replay, more reliable delivery**.
* instead of directly writing to sqs queue from lambda, i can probably lambda -> save to s3 -> trigger event either directly to lambda or into an sqs queue

[s3 performance](https://www.udemy.com/course/aws-certified-solutions-architect-associate-saa-c03/learn/lecture/18078239#lecture-article)
* each prefix in an S3 bucket supports:
    * 3,500 PUT, COPY, POST, or DELETE requests per second
    * 5,500 GET or HEAD requests per second
* ***s3 transfer acceleration*** -  **increases upload and download speeds by routing data through AWS edge locations**. basically within aws it's fast, minimise edge latency so can reduce bottleneck
* ***s3 byte range fetches*** - allows parallelisation / partial retrieval of the file / speed up downloads

[s3 batch operations](https://www.udemy.com/course/aws-certified-solutions-architect-associate-saa-c03/learn/lecture/33878726#lecture-article)
* WHAT
    * Modify all the object metadata and properties of many S3 objects at once.
    * Copy objects between S3 buckets as a batch operation.
    * Encrypt all unencrypted objects in your S3 buckets.
    * Modify Access Control Lists (ACLs) or tags for multiple objects.
    * Restore many objects simultaneously from S3 Glacier.
    * Invoke a Lambda function to perform custom actions on every object in the batch.
* Why use S3 Batch Operations instead of scripting these tasks yourself?
    * Management of retries is handled automatically.
    * You can track the progress of the batch job.
    * Completion notifications can be sent upon job completion.
    * Detailed reports are generated for auditing and verification.
* S3 inveotry and then athena to get list of objects

[s3 storage lens](https://www.udemy.com/course/aws-certified-solutions-architect-associate-saa-c03/learn/lecture/42878778#lecture-article)
* got default dashboard, can export reports / metrics in csv/parrquet and available for different retention period
* 28 free metrics available, rest paid. default retention 14 days, paid retention up to 15 months

[s3 encryption](https://www.udemy.com/course/aws-certified-solutions-architect-associate-saa-c03/learn/lecture/13528318#overview) + [hands on](https://www.udemy.com/course/aws-certified-solutions-architect-associate-saa-c03/learn/lecture/23743320#lecture-article)
* Server-side encryption (SSE), which has multiple flavors:
    * SSE-S3: Server-side encryption with Amazon S3-managed keys. Never have access to key, AES-256. Set header `x-amz-server-side-encryption: AES256`. Enabled by default for new buckets and objects
    * SSE-KMS: Server-side encryption with AWS Key Management Service (KMS) keys. Set header  `x-amz-server-side-encryption: aws:kms`. 
        * When you upload or download files from Amazon S3, you need to leverage a KMS key, which has its own APIs such as GenerateDataKey and Decrypt. Each API call counts towards the KMS quotas of API calls per second, which ranges between 5,000 and 30,000 requests per second depending on the region. These quotas can be increased using the Service Quotas Console.
    * SSE-C: Server-side encryption with customer-provided keys. After use, key is discarded by AWS. Need to keep the key yourself to decrypt stuff. Must use HTTPS obvs
* Client-side encryption, where you encrypt everything client-side before uploading to Amazon S3.
* Encryption in transit (HTTPS) - can be forced with bucket policy

```
{
  "Version": "2012-10-17",
  "Statement": [
    {
      "Effect": "Deny",
      "Principal": "*",
      "Action": "s3:GetObject",
      "Resource": "arn:aws:s3:::YOUR_BUCKET_NAME/*",
      "Condition": {
        "Bool": {"aws:SecureTransport": "false"}
      }
    }
  ]
}
```

[s3 default encryption](https://www.udemy.com/course/aws-certified-solutions-architect-associate-saa-c03/learn/lecture/13531242#lecture-article)
* **By default, all S3 buckets have server-side encryption with Amazon S3-managed keys (SSE-S3) enabled. This default encryption is automatically applied to new objects uploaded to these buckets.**
* Bucket policy that can deny things not appropriately encrypted - policies always evaluated before default encryption settings

[cross origin resource sharing](https://www.udemy.com/course/aws-certified-solutions-architect-associate-saa-c03/learn/lecture/13528324#lecture-article) + [hands on](https://www.udemy.com/course/aws-certified-solutions-architect-associate-saa-c03/learn/lecture/19736418#lecture-article)
* An origin is defined by a scheme (protocol), host (domain), and port. For example, consider the URL https://www.example.com. Here, the implied port is 443 for HTTPS, the protocol is HTTPS itself, and the domain is www.example.com.
* Same origin, same scheme + host + port. **requests between different origins require explicit permission via CORS headers.**
* **When a client makes a cross-origin request to an Amazon S3 bucket, the bucket must have the correct CORS headers enabled to allow the request. This is a common exam question.**

[mfa delete](https://www.udemy.com/course/aws-certified-solutions-architect-associate-saa-c03/learn/lecture/13528342#lecture-article) + [hands on](https://www.udemy.com/course/aws-certified-solutions-architect-associate-saa-c03/learn/lecture/23743360#lecture-article)
* generates a code that must be entered into Amazon S3 before performing important operations, such as permanently deleting object versions or suspending versioning
* enabling MFA delete **must be done using aws cli**

[s3 access logs](https://www.udemy.com/course/aws-certified-solutions-architect-associate-saa-c03/learn/lecture/13714590#lecture-article) + [hands on](https://www.udemy.com/course/aws-certified-solutions-architect-associate-saa-c03/learn/lecture/23743440#lecture-article)
* target logging buckets must be in the same AWS region as the buckets being monitored, enable access logging, specific format for these logs - [https://docs.aws.amazon.com/AmazonS3/latest/userguide/LogFormat.html](https://docs.aws.amazon.com/AmazonS3/latest/userguide/LogFormat.html)
* don't have logging bucket same as monitored bucket - it will create a loop and you will die

[s3 pre-signed urls](https://www.udemy.com/course/aws-certified-solutions-architect-associate-saa-c03/learn/lecture/13528348#lecture-article) + [hands on](https://www.udemy.com/course/aws-certified-solutions-architect-associate-saa-c03/learn/lecture/23743384#lecture-article)
*  user who receives that URL inherits the permissions of the user who generated it. This applies for both GET and PUT operations
* cli up to 168 hours, console up to 12 hours expiry
* non public bucket can haz shared file with everyone for a period of time

[s3 glacier vault lock](https://www.udemy.com/course/aws-certified-solutions-architect-associate-saa-c03/learn/lecture/18078251#lecture-article)
* write once read many - create a vault lock policy, lock the policy for future edits
* **s3 object lock** - similar but not quite the same. **compliance mode** - cannot overwrite or delete by ANY user, no change nuffing; **governance mode** - iam can grant some users special permission to do stuff. also can place a legal hold, with user iam permission `s3:PutObjectLegalHold`

[s3 access points](https://www.udemy.com/course/aws-certified-solutions-architect-associate-saa-c03/learn/lecture/33878720#lecture-article)
* cos bucket policy suckssssss and is hard to scale for complexity.... security management is shifted from a single bucket policy to multiple access point policies
* access point policy, which resembles an S3 bucket policy and grants read and write access to the finance prefix
* points can be defined to be privately accessible through a VPC origin; VPC endpoint has its own policy which must allow access to target buckets and access points

[s3 object lambda](https://www.udemy.com/course/aws-certified-solutions-architect-associate-saa-c03/learn/lecture/36228836#lecture-article)
* modify data dynamically without duplicating storage
* uses access points connected to lambda, use cases: 
    * Redacting personally identifiable information (PII) for analytics or non-production environments.
    * Converting data formats, for example, from XML to JSON.
    * Performing any kind of transformation, such as resizing and watermarking images on the fly, where the watermark is specific to the user who requests the object.
